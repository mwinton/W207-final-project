{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Notebook\n",
    "[Return to project overview](final_project_overview.ipynb)\n",
    "\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import util\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split class labels into separate array\n",
    "\n",
    "Our utility function reads the merged dataset, imputes the column mean for missing numeric values, and then performs a stratified train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = util.read_data(do_imputation=True)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: a hypothetical model that is hard-coded to predict a `negative` result every time would be ~77% accurate.  So, we should not accept any machine-learned model with a lower accuracy than that.  This also suggests that F1 score is a better metric to assess our work since it incorporates both precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: the feature `school_income_estimate` only has non-null values for 104 of 371 records in the training data.  We should drop it from further analysis, as imputing its value for the non-null records isn't appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to estimate MLP models and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mlp(train_data, train_labels, n_pca=None,\n",
    "                 hidden_layers=None, k_folds=5, max_iter=1000, print_results=True):\n",
    "\n",
    "    # if tuple describing hidden layer nodes isn't provided, set default\n",
    "    if not hidden_layers:\n",
    "        n_features = train_data.shape[1]\n",
    "        hidden_layers = (n_features,n_features,n_features)\n",
    "        \n",
    "    # build pipelines, with or without PCA as appropriate\n",
    "    if n_pca:\n",
    "        # create a pipeline to run StandardScaler and MLP\n",
    "        print('Estimating pipeline with PCA; hidden layers:',hidden_layers)\n",
    "        pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                                 PCA(n_components=n_pca, random_state=207),\n",
    "                                 MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                                               max_iter=max_iter, random_state=207))\n",
    "    else:\n",
    "        # create a pipeline to run StandardScaler and MLP\n",
    "        print('Estimating pipeline without PCA; hidden layers:',hidden_layers)\n",
    "        pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                                 MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                                               max_iter=max_iter, random_state=207))\n",
    "\n",
    "    # Do k-fold cross-validation, collecting both \"test\" accuracy and F1 \n",
    "    cv_scores = cross_validate(pipeline, train_data, train_labels, cv=k_folds, scoring=['accuracy','f1'])\n",
    "    if print_results:\n",
    "        util.print_cv_results(cv_scores)\n",
    "        \n",
    "    # extract and return accuracy, F1\n",
    "    cv_accuracy = cv_scores['test_accuracy']\n",
    "    cv_f1 = cv_scores['test_f1']\n",
    "    return (cv_accuracy.mean(), cv_accuracy.std(), cv_f1.mean(), cv_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and fit a \"naive\" model\n",
    "For the first model, we'll use all features except SHSAT-related features because they are too correlated with the way we calculated the label.  We'll also drop `school_income_estimate` because it's missing for ~2/3 of the schools.  We drop zip code (too granular to have many schools per zip) in favor of the indicator variables `in_[borough]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "             'zip'\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns\n",
    "train_data_naive = train_data.drop(drop_cols, axis=1)\n",
    "test_data_naive = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "print(train_data_naive.shape)\n",
    "train_data_naive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode the categorical explanatory variables\n",
    "Columns such as zip code and school district ID, which are integers should not be fed into an ML model as integers.  Instead, we would need to treat them as factors and perform one-hot encoding.  Since we have already removed zip code from our dataframe (in favor of boroughs), we only need to one hot encode `district`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_naive_ohe, test_data_naive_ohe = util.get_dummies(train_data_naive, test_data_naive,\n",
    "                                                             factor_cols=['district'])\n",
    "train_data_naive_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"naive\" multilayer perceptron model\n",
    "This first \"naive\" model uses all except for the SHSAT-related features, as described above.  We create a pipeline that will be used for k-fold cross-validation.  First, we scale the features, then estimate a multilayer perceptron neural network with 3 hidden layers, each with the same number of nodes as we have features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_ohe, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"naive\" model without location (zip, borough, or district)\n",
    "Next, we will remove the borough and district features and compare accuracy to the model that included one hot-encoded versions of the borough and district factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate',\n",
    "             'district',\n",
    "             'zip',\n",
    "             'in_bronx',\n",
    "             'in_brooklyn',\n",
    "             'in_manhattan',\n",
    "             'in_queens',\n",
    "             'in_staten'\n",
    "            ]\n",
    "\n",
    "# drop SHSAT-related columns + district, zip, borough\n",
    "train_data_naive_nozip = train_data.drop(drop_cols, axis=1)\n",
    "test_data_naive_nozip = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "print(train_data_naive_nozip.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"naive\" multilayer perceptron model without location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_nozip, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: while the accuracy is similar when we exclude zip code and school district, the F1 score is lower.  This suggests that it's important to keep these factors in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a \"race-blind\" multilayer perceptron model\n",
    "Because we know there's an existing bias problem in the NYC schools, in that the demographics of the test taking population have been getting more homogenous, and the explicit goal of PASSNYC is to make the pool more diverse, we want to train a model that excludes most demographic features.  This would enable us to train a \"race-blind\" model.  \n",
    "\n",
    "### Preprocess new X_train and X_test datasets\n",
    "We will remove all explicitly demographic columns, as well as economic factors, borough, and zip code, which are likely highly correlated with demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SHSAT-related columns\n",
    "drop_cols = ['dbn',\n",
    "             'num_shsat_test_takers',\n",
    "             'offers_per_student',\n",
    "             'pct_test_takers',\n",
    "             'school_name',\n",
    "             'school_income_estimate'\n",
    "            ]\n",
    "train_data_race_blind = train_data.drop(drop_cols, axis=1)\n",
    "test_data_race_blind = test_data.drop(drop_cols, axis=1)\n",
    "\n",
    "# drop additional (demographic) columns\n",
    "race_cols = ['percent_ell',\n",
    "             'percent_asian',\n",
    "             'percent_black',\n",
    "             'percent_hispanic',\n",
    "             'percent_black__hispanic',\n",
    "             'percent_white',\n",
    "             'economic_need_index',\n",
    "             'zip',\n",
    "             'in_bronx',\n",
    "             'in_brooklyn',\n",
    "             'in_manhattan',\n",
    "             'in_queens',\n",
    "             'in_staten'\n",
    "             ]\n",
    "train_data_race_blind = train_data_race_blind.drop(race_cols, axis=1)\n",
    "test_data_race_blind = test_data_race_blind.drop(race_cols, axis=1)\n",
    "\n",
    "# one-hot encode these features as factors\n",
    "factor_cols = ['district']\n",
    "train_data_race_blind_ohe, test_data_race_blind_ohe =util.get_dummies(train_data_race_blind,\n",
    "                                                                      test_data_race_blind, factor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the \"race blind\" multilayer perceptron model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_race_blind_ohe, train_labels, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **KEY OBSERVATION**: the F1 score for the race-blind model declines further when we remove these features.  Of the models we have tested, the original \"naive\" model (with the most features) performs better than our race-blind model, or our model that excluded only zip and district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with dimensionality reduction via PCA\n",
    "Since manual feature selection performed poorly, resulting in a confidence interval of F1 spanning from 0 to 1 in both cases, it doesn't seem to be a promising approach.  Next, we experiment with Principal Component Analysis for dimensionality reduction, starting with the \"naive\" set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of principal components to achieve 90% explained variance\n",
    "n_pca = util.get_num_pcas(train_data_naive, var_explained=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using %d principal components' % (n_pca)) # currently n_pca=69\n",
    "\n",
    "# discard return vals; only print results\n",
    "(_,_,_,_) = estimate_mlp(train_data_naive_ohe, train_labels, n_pca=n_pca, k_folds=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use grid search to identify best set of hidden layer parameters\n",
    "Since the usage of PCA seemed to improve our F1 score (and tighten its confidence interval), we will proceed to try to optimize the hidden layer parameters while using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running grid search for different combinations of neural network parameters is slow.\n",
    "# If results already exist as a file, load them instead of re-running.\n",
    "try:\n",
    "    grid_search_results = pd.read_csv('cache_neuralnet/gridsearch_results.csv')\n",
    "    print('Loaded grid search results from file.')\n",
    "except FileNotFoundError:\n",
    "    print('Performing grid search for best hidden layer parameters.')\n",
    "\n",
    "    # We'll time it and report how long it took to run:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # numbers of hidden nodes = these multipliers * # features\n",
    "    n_features = train_data_naive_ohe.shape[1]\n",
    "#     fraction = [0.25, 0.5]\n",
    "    fraction = [0.25, 0.5, 1.0, 1.5, 2.0]\n",
    "    n_layer_features = (int(f * n_features) for f in fraction)\n",
    "    n_nodes = list(n_layer_features)\n",
    "\n",
    "    # create list of tuples of hidden layer param permutations\n",
    "    # only explore up to 4 hidden layers\n",
    "    hl_param_candidates = []\n",
    "    for h1 in n_nodes:\n",
    "        hl_param_candidates.append((h1))\n",
    "        for h2 in n_nodes:\n",
    "            hl_param_candidates.append((h1,h2))\n",
    "            for h3 in n_nodes:\n",
    "                hl_param_candidates.append((h1,h2,h3))\n",
    "                for h4 in n_nodes:\n",
    "                    hl_param_candidates.append((h1,h2,h3,h4))\n",
    "    \n",
    "    # train an MLP model and perform cross-validation for each parameter set\n",
    "    print('Estimating %d MLP models. This will take time!\\n' % (len(hl_param_candidates)))\n",
    "    tmp_results = []        \n",
    "    for hl in hl_param_candidates:\n",
    "        tmp_acc, tmp_acc_std, tmp_f1, tmp_f1_std = estimate_mlp(train_data_naive_ohe, train_labels, \n",
    "                                                                hidden_layers=hl, n_pca=n_pca,\n",
    "                                                                k_folds=5, max_iter=1000, print_results=False)\n",
    "        tmp_results.append((hl, tmp_acc, tmp_acc - 1.96 * tmp_acc_std, tmp_acc + 1.96 * tmp_acc_std,\n",
    "                                    tmp_f1, tmp_f1 - 1.96 * tmp_f1_std, tmp_f1 + 1.96 * tmp_f1_std))\n",
    "\n",
    "    # calculated elapsed time\n",
    "    end_time = time.time()\n",
    "    took = int(end_time - start_time)\n",
    "    print(\"Grid search took {0:d} minutes, {1:d} seconds.\".format(took // 60, took % 60))\n",
    "\n",
    "    # convert results to a dataframe for easier display\n",
    "    grid_search_results = pd.DataFrame(tmp_results)\n",
    "    grid_search_results.columns=(['Hidden Layers','Accuracy','Acc Lower CI', 'Acc Upper CI','F1','F1 Lower CI','F1 Upper CI'])\n",
    "    grid_search_results.to_csv('cache_neuralnet/gridsearch_results.csv', index=False)\n",
    "\n",
    "# Display grid search results\n",
    "grid_search_results.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put best grid search params into a varaiable\n",
    "best_param_idx = grid_search_results['F1'].idxmax()\n",
    "\n",
    "try:  # this is needed when loading from file\n",
    "    best_hl_params = eval(grid_search_results['Hidden Layers'][best_param_idx])\n",
    "except TypeError:  # eval isn't needed when results are still in memory\n",
    "    best_hl_params = grid_search_results['Hidden Layers'][best_param_idx]\n",
    "    \n",
    "best_hl_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate \"out-of-sample\" test set accuracy\n",
    "At this point we can use our \"best\" model parameters to classify our test set, and compare to true labels.\n",
    "\n",
    "> NOTE: This code was left commented out until after hyperparameter optimization was complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline with optimal parameters\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                     PCA(n_components=n_pca, random_state=207),\n",
    "                     MLPClassifier(hidden_layer_sizes=best_hl_params,\n",
    "                                   max_iter=1000, random_state=207))\n",
    "pipeline.fit(train_data_naive_ohe, train_labels)\n",
    "test_predict = pipeline.predict(test_data_naive_ohe)\n",
    "\n",
    "print('Test set accuracy: %.2f\\n' % (np.mean(test_predict==test_labels)))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(test_labels, test_predict)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('True negatives: %d' % (tn))\n",
    "print('True positives: %d' % (tp))\n",
    "print('False negatives: %d' % (fn))\n",
    "print('False positives: %d\\n' % (fp))\n",
    "print(classification_report(test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze false positives to make recommendations to PASSNYC\n",
    "False positives are the schools that our model predicted to have a high SHSAT registration rate, but in reality they did not.  This suggests that they have a lot in common with the high registration schools, but for some reason fall short.  As a result, we believe these are good candidates for the PASSNYC organization to engage with, as investing in programs with these schools may be more highly to payoff with increase registration rates.  We will prioritize the schools based on features that align with the PASSNYC diversity-oriented mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombine train and test data into an aggregate dataset\n",
    "X_orig = pd.concat([train_data, test_data], sort=True)  # including all columns (need for display purposes)\n",
    "X_best = pd.concat([train_data_naive_ohe, test_data_naive_ohe], sort=True)  # only columns from best model\n",
    "y = np.concatenate((train_labels,test_labels))\n",
    "\n",
    "# Run k-fold cross-validation with 5 folds 10 times, which means every school is predicted 10 times.\n",
    "folds = 5\n",
    "repeats = 10\n",
    "rskf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=repeats, random_state=207)\n",
    "\n",
    "# Build dataframes for storing predictions, with columns for each k-fold\n",
    "fold_list = []\n",
    "for f in range(1, (folds * repeats) + 1):\n",
    "    fold_list.append('k{}'.format(f))\n",
    "predictions = pd.DataFrame(index=X_best.index, columns=fold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the Repeated Stratified K Fold, and and fill out the DataFrames\n",
    "counter = 1\n",
    "print('Please be patient...')\n",
    "for train, test in rskf.split(X_best, y):\n",
    "    pipeline = make_pipeline(StandardScaler(with_mean=False), \n",
    "                         PCA(n_components=n_pca, random_state=207),\n",
    "                         MLPClassifier(hidden_layer_sizes=best_hl_params,\n",
    "                                       max_iter=1000, random_state=207))\n",
    "    pipeline.fit(X_best.iloc[train,], y[train,])\n",
    "    predicted_labels = pipeline.predict(X_best.iloc[test,])\n",
    "    predictions.iloc[test, counter-1] = predicted_labels\n",
    "    counter += 1\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for predictions and labels\n",
    "predictions['1s'] = predictions.iloc[:,:50].sum(axis=1)\n",
    "predictions['0s'] = (predictions.iloc[:,:50]==0).sum(axis=1)\n",
    "predictions['true'] = y\n",
    "\n",
    "# Create a table of all features along with the number of votes each received and the true value\n",
    "X_predicted = pd.concat([X_orig, predictions['1s'], predictions['0s'],\n",
    "                         pd.DataFrame(y)], axis=1, join_axes=[X_best.index])\n",
    "\n",
    "# Rename the y columns to be more descriptive\n",
    "X_predicted.rename(columns={0:\"high_registrations\"}, inplace=True)\n",
    "\n",
    "# Sort by number of votes, most votes for 1 at the top and most votes for 0 at the bottom\n",
    "X_predicted = X_predicted.sort_values(by=['1s', '0s'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the columns of interest for PASSNYC prioritization\n",
    "final_features = ['1s',\n",
    "                  'dbn',\n",
    "                  'school_name',\n",
    "                  'economic_need_index',\n",
    "                  'grade_7_enrollment',\n",
    "                  'num_shsat_test_takers',\n",
    "                  'pct_test_takers',\n",
    "                  'percent_black__hispanic'\n",
    "              ]\n",
    "df_passnyc = X_predicted[final_features]\n",
    "\n",
    "# Determine the number of test takers this school would have needed to meet\n",
    "#   the median percentage of high_registrations\n",
    "median_pct = np.median(X_orig[y==1]['pct_test_takers'])/100\n",
    "target_test_takers = np.multiply(df_passnyc['grade_7_enrollment'], median_pct)\n",
    "\n",
    "# Subtract the number of actual test takers from the hypothetical minimum number\n",
    "delta = target_test_takers - df_passnyc['num_shsat_test_takers']\n",
    "\n",
    "# Multiply the delta by the minority percentage of the school to determine how many minority\n",
    "#   students did not take the test\n",
    "minority_delta = np.round(np.multiply(delta, df_passnyc['percent_black__hispanic']/100),0).astype(int)\n",
    "df_passnyc = df_passnyc.assign(minority_delta=minority_delta)\n",
    "\n",
    "# Multiply the minority delta by the percentage of 1 votes to get a confidence-adjusted 'score'\n",
    "df_passnyc = df_passnyc.assign(score=np.multiply(df_passnyc['minority_delta'], df_passnyc['1s']/10))\n",
    "\n",
    "# sort descending, and filter to schools with more than five minority students\n",
    "df_passnyc = df_passnyc.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Create a rank order column\n",
    "df_passnyc.insert(0, 'rank', range(1,df_passnyc.shape[0]+1))\n",
    "\n",
    "# Write to CSV\n",
    "df_passnyc.to_csv('results/results.neuralnet.csv')\n",
    "\n",
    "df_passnyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc comparison of prioritization score vs. economic need index\n",
    "Even though economic need index was not an explicit factor in our post-classification prioritization scoring/ranking system, it is interesting to observe that there is some correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_passnyc['score']\n",
    "y = df_passnyc['economic_need_index']\n",
    "sns.regplot(x='score', y='economic_need_index', data=df_passnyc,\n",
    "           fit_reg=True, x_jitter=1, scatter_kws={'alpha': 0.5, 's':4})\n",
    "# plt.scatter(x, y)\n",
    "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "plt.xlabel('PASSNYC Priorization \"Score\"')\n",
    "plt.ylabel('Economic Need Index')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
