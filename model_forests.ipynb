{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Notebook\n",
    "[Return to project overview](final_project_overview.ipynb),\n",
    "\n",
    "### Andrew Larimer, Deepak Nagaraj, Daniel Olmstead, Michael Winton (W207-4-Summer 2018 Final Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setting options\n",
    "\n",
    "First we import necessary libraries, including our util functions, and set Pandas and Matplotlib options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import graphviz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from util import our_train_test_split, read_data, get_dummies, \\\n",
    "    print_cv_results, run_model_get_ordered_predictions, create_passnyc_list\n",
    "import pickle\n",
    "\n",
    "# set default options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our data using util cleanup and imputing\n",
    "\n",
    "Our util module has shared utility functions for cleaning up our data and imputing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 371 observations (positive class fraction: 0.232)\n",
      "Test : 93 observations (positive class fraction: 0.226)\n"
     ]
    }
   ],
   "source": [
    "# Read the cleaned, merged, and mean-imputed data from our utility function\n",
    "train_data, test_data, train_labels, test_labels = read_data(do_imputation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop some columns that were used to calculate our dependendt variable, as well as our index column, school name strings, and `school_income_estimate` which had too many missing values to fill via imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We drop a few features for the following reasons:\n",
    "#    Used in generating dependent variable: 'num_shsat_test_takers',\n",
    "#        'offers_per_student', 'pct_test_takers'\n",
    "#    Strings or other non-features: 'dbn', 'school_name'\n",
    "#    Too many empty values: 'school_income_estimate'\n",
    "#    Data preserved in other features: 'zip', 'rigorous_instruction_rating',\n",
    "#       'collaborative_teachers_rating', 'supportive_environment_rating',\n",
    "#       'effective_school_leadership_rating',\n",
    "#       'strong_family_community_ties_rating', 'trust_rating'\n",
    "#    Found not to help model: 'district' (or one-hot encoding)\n",
    "\n",
    "FEATURES_TO_DROP = ['dbn', 'school_name', 'zip', 'num_shsat_test_takers',\n",
    "                    'offers_per_student', 'pct_test_takers', 'school_income_estimate',\n",
    "                    'rigorous_instruction_rating','collaborative_teachers_rating',\n",
    "                    'supportive_environment_rating',\n",
    "                    'effective_school_leadership_rating',\n",
    "                    'strong_family_community_ties_rating', 'trust_rating',\n",
    "                    'district']\n",
    "\n",
    "# We'll go ahead and drop total_columns_to_drop columns.\n",
    "train_prepped = train_data.drop(FEATURES_TO_DROP,axis=1)\n",
    "test_prepped = test_data.drop(FEATURES_TO_DROP,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirm total of remaining NAs is:  0\n"
     ]
    }
   ],
   "source": [
    "# We confirm our resulting data has no more NAs\n",
    "print(\"Confirm total of remaining NAs is: \",np.sum(np.sum(train_prepped.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Random Forest Model on Cross-Validation\n",
    "\n",
    "We now move into training our random forest model. To optimize our hyperparameter of how many trees to include in our forest, we use GridSearchCV and take advantage of its cross validation capability to use cross validation against our training set instead of further reducing our data into smaller train and dev sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>split4_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>266</td>\n",
       "      <td>0.165147</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.212079</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.686647</td>\n",
       "      <td>0.070353</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.846361</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>0.164223</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.211624</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.686647</td>\n",
       "      <td>0.070353</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.846361</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>0.165117</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.214894</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.8, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.683964</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.838275</td>\n",
       "      <td>0.038780</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>314</td>\n",
       "      <td>0.163819</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.211772</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.8, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.683964</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.838275</td>\n",
       "      <td>0.038780</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>0.163367</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.213975</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.8, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.683964</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.838275</td>\n",
       "      <td>0.038780</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>310</td>\n",
       "      <td>0.170330</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.212620</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.8, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.069407</td>\n",
       "      <td>6</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.843666</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>0.166698</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.210950</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.683418</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>7</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.824798</td>\n",
       "      <td>0.056495</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>0.166968</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.210940</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.683418</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>7</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.824798</td>\n",
       "      <td>0.056495</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>0.166740</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.210377</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.683418</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>7</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.824798</td>\n",
       "      <td>0.056495</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>0.164173</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.213329</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.5, 'min_sam...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.683172</td>\n",
       "      <td>0.068446</td>\n",
       "      <td>10</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.843666</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "266         266       0.165147      0.001103         0.212079        0.002194   \n",
       "270         270       0.164223      0.001474         0.211624        0.002179   \n",
       "322         322       0.165117      0.001801         0.214894        0.002080   \n",
       "314         314       0.163819      0.001699         0.211772        0.002795   \n",
       "318         318       0.163367      0.001009         0.213975        0.002829   \n",
       "310         310       0.170330      0.006534         0.212620        0.001523   \n",
       "170         170       0.166698      0.001896         0.210950        0.002953   \n",
       "174         174       0.166968      0.001583         0.210940        0.002204   \n",
       "178         178       0.166740      0.000333         0.210377        0.000891   \n",
       "274         274       0.164173      0.001499         0.213329        0.001854   \n",
       "\n",
       "     param_max_depth  param_max_features  param_min_samples_leaf  \\\n",
       "266              5.0                 0.5                       2   \n",
       "270              5.0                 0.5                       2   \n",
       "322              5.0                 0.8                       4   \n",
       "314              5.0                 0.8                       4   \n",
       "318              5.0                 0.8                       4   \n",
       "310              5.0                 0.8                       2   \n",
       "170              2.0                 0.5                       4   \n",
       "174              2.0                 0.5                       4   \n",
       "178              2.0                 0.5                       4   \n",
       "274              5.0                 0.5                       2   \n",
       "\n",
       "     param_min_samples_split  param_n_estimators  \\\n",
       "266                        2                 100   \n",
       "270                        3                 100   \n",
       "322                        5                 100   \n",
       "314                        2                 100   \n",
       "318                        3                 100   \n",
       "310                        5                 100   \n",
       "170                        2                 100   \n",
       "174                        3                 100   \n",
       "178                        5                 100   \n",
       "274                        5                 100   \n",
       "\n",
       "                                                params  split0_test_f1  \\\n",
       "266  {'max_depth': 5, 'max_features': 0.5, 'min_sam...        0.787879   \n",
       "270  {'max_depth': 5, 'max_features': 0.5, 'min_sam...        0.787879   \n",
       "322  {'max_depth': 5, 'max_features': 0.8, 'min_sam...        0.787879   \n",
       "314  {'max_depth': 5, 'max_features': 0.8, 'min_sam...        0.787879   \n",
       "318  {'max_depth': 5, 'max_features': 0.8, 'min_sam...        0.787879   \n",
       "310  {'max_depth': 5, 'max_features': 0.8, 'min_sam...        0.787879   \n",
       "170  {'max_depth': 2, 'max_features': 0.5, 'min_sam...        0.800000   \n",
       "174  {'max_depth': 2, 'max_features': 0.5, 'min_sam...        0.800000   \n",
       "178  {'max_depth': 2, 'max_features': 0.5, 'min_sam...        0.800000   \n",
       "274  {'max_depth': 5, 'max_features': 0.5, 'min_sam...        0.787879   \n",
       "\n",
       "     split1_test_f1  split2_test_f1  split3_test_f1  split4_test_f1  \\\n",
       "266        0.666667        0.666667        0.578947        0.731707   \n",
       "270        0.666667        0.666667        0.578947        0.731707   \n",
       "322        0.666667        0.666667        0.615385        0.681818   \n",
       "314        0.666667        0.666667        0.615385        0.681818   \n",
       "318        0.666667        0.666667        0.615385        0.681818   \n",
       "310        0.687500        0.648649        0.578947        0.714286   \n",
       "170        0.722222        0.682927        0.585366        0.625000   \n",
       "174        0.722222        0.682927        0.585366        0.625000   \n",
       "178        0.722222        0.682927        0.585366        0.625000   \n",
       "274        0.666667        0.666667        0.578947        0.714286   \n",
       "\n",
       "     mean_test_f1  std_test_f1  rank_test_f1  split0_test_accuracy  \\\n",
       "266      0.686647     0.070353             1              0.906667   \n",
       "270      0.686647     0.070353             1              0.906667   \n",
       "322      0.683964     0.056932             3              0.906667   \n",
       "314      0.683964     0.056932             3              0.906667   \n",
       "318      0.683964     0.056932             3              0.906667   \n",
       "310      0.683734     0.069407             6              0.906667   \n",
       "170      0.683418     0.075142             7              0.906667   \n",
       "174      0.683418     0.075142             7              0.906667   \n",
       "178      0.683418     0.075142             7              0.906667   \n",
       "274      0.683172     0.068446            10              0.906667   \n",
       "\n",
       "     split1_test_accuracy  split2_test_accuracy  split3_test_accuracy  \\\n",
       "266              0.864865              0.824324              0.783784   \n",
       "270              0.864865              0.824324              0.783784   \n",
       "322              0.851351              0.824324              0.797297   \n",
       "314              0.851351              0.824324              0.797297   \n",
       "318              0.851351              0.824324              0.797297   \n",
       "310              0.864865              0.824324              0.783784   \n",
       "170              0.864865              0.824324              0.770270   \n",
       "174              0.864865              0.824324              0.770270   \n",
       "178              0.864865              0.824324              0.770270   \n",
       "274              0.864865              0.824324              0.783784   \n",
       "\n",
       "     split4_test_accuracy  mean_test_accuracy  std_test_accuracy  \\\n",
       "266              0.851351            0.846361           0.041066   \n",
       "270              0.851351            0.846361           0.041066   \n",
       "322              0.810811            0.838275           0.038780   \n",
       "314              0.810811            0.838275           0.038780   \n",
       "318              0.810811            0.838275           0.038780   \n",
       "310              0.837838            0.843666           0.041093   \n",
       "170              0.756757            0.824798           0.056495   \n",
       "174              0.756757            0.824798           0.056495   \n",
       "178              0.756757            0.824798           0.056495   \n",
       "274              0.837838            0.843666           0.041093   \n",
       "\n",
       "     rank_test_accuracy  \n",
       "266                  26  \n",
       "270                  26  \n",
       "322                  87  \n",
       "314                  87  \n",
       "318                  87  \n",
       "310                  40  \n",
       "170                 283  \n",
       "174                 283  \n",
       "178                 283  \n",
       "274                  40  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define into how many groups we'd like to split our test data\n",
    "# for use in cross-validation to evaluate hyperparameters.\n",
    "KFOLDS = 5\n",
    "\n",
    "# We check for previously saved results and a serialized model saved\n",
    "# to disk before re-running GridSearchCV. To force it to run again,\n",
    "# we can comment out the try: & except: or just delete the last saved\n",
    "# results.\n",
    "\n",
    "try:\n",
    "\n",
    "    cv_results = pd.read_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    with open('cache_forest/pickled_forest','rb') as f:\n",
    "        forest_cv = pickle.load(f)\n",
    "\n",
    "except:\n",
    "\n",
    "    # If no saved results are found, we define our base Random Forest\n",
    "    # Classifier with fixed parameters we don't anticipate adjusting.\n",
    "    # We want to run as many jobs as we have cores at once (which is\n",
    "    # what the -1 input to n_jobs does, and we define our random state\n",
    "    # for reproducibility.)\n",
    "    forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced',\n",
    "                                    random_state=207)\n",
    "\n",
    "    # We define a range of paramters we'd like to try for our forest.\n",
    "    params_to_try = {'n_estimators':[10,30,100,300],\n",
    "                     'max_depth':[None,2,5,7],\n",
    "                     'min_samples_leaf':[1,2,4],\n",
    "                     'min_samples_split':[2,3,5],\n",
    "                     'max_features':[.2,.5,.8]}\n",
    "\n",
    "    # Now we run GridSearchCV on our forest estimator, trying our varying\n",
    "    # numbers of trees and utilizing our cross validation to determine the\n",
    "    # best number of trees across the best number of train/dev cross\n",
    "    # validation splits, using a weighted F1 score as our metric of success.\n",
    "    forest_cv = GridSearchCV(forest, params_to_try, scoring=['f1',\n",
    "                            'accuracy'], refit='f1', cv=KFOLDS,\n",
    "                             return_train_score=False)\n",
    "\n",
    "    # We'll time it and report how long it took to run:\n",
    "    start_time = time.time()\n",
    "    forest_cv.fit(train_prepped, train_labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    took = int(end_time - start_time)\n",
    "    print(\"Grid search took {0:d} minutes, {1:d} seconds.\".format(\n",
    "              took // 60, took % 60))\n",
    "\n",
    "    # And pickle our trained model, and save our scores to csv.\n",
    "    with open('cache_forest/pickled_forest','wb') as f:\n",
    "        pickle.dump(forest_cv, f)\n",
    "\n",
    "    cv_results = pd.DataFrame(forest_cv.cv_results_)\n",
    "    cv_results.to_csv('cache_forest/forest_gridsearch_results.csv')\n",
    "    \n",
    "# Then display our results in a Pandas dataframe, sorted by\n",
    "# rank based on mean f1 score across 5-fold CV testing:\n",
    "cv_results.sort_values('rank_test_f1').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "\n",
      "max_depth : 5\n",
      "max_features : 0.5\n",
      "min_samples_leaf : 2\n",
      "min_samples_split : 2\n",
      "n_estimators : 100\n",
      "\n",
      "\n",
      "With 5-fold cross-validation,\n",
      "Accuracy is: 0.846 (95% CI from 0.766 to 0.927).\n",
      "The F1 score is: 0.687 (95% CI from 0.549 to 0.825).\n"
     ]
    }
   ],
   "source": [
    "# We extract our best model and best parameters from our GridSearchCV results.\n",
    "best_forest = forest_cv.best_estimator_\n",
    "best_params = forest_cv.best_params_\n",
    "\n",
    "# We reiterate our preferred number of cross validation folds if we haven't\n",
    "# had to re-train our model\n",
    "KFOLDS = 5\n",
    "\n",
    "print(\"Best params:\\n\")\n",
    "for param, val in best_params.items():\n",
    "    print(param,':',val)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "winning_cv_results = cv_results[cv_results['rank_test_f1'] == 1].iloc[1,:]\n",
    "\n",
    "# display accuracy with 95% confidence interval\n",
    "winning_mean_accuracy = winning_cv_results['mean_test_accuracy']\n",
    "std_accuracy = winning_cv_results['std_test_accuracy']\n",
    "print('With %d-fold cross-validation,\\nAccuracy is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (KFOLDS, winning_mean_accuracy,\n",
    "           float(winning_mean_accuracy - 1.96 * std_accuracy),\n",
    "           float(winning_mean_accuracy + 1.96 * std_accuracy)))\n",
    "\n",
    "# display F1 score with 95% confidence interval\n",
    "winning_mean_f1 = winning_cv_results['mean_test_f1']\n",
    "std_f1 = winning_cv_results['std_test_f1']\n",
    "print('The F1 score is: %.3f (95%% CI from %.3f to %.3f).' %\n",
    "          (winning_mean_f1,\n",
    "           float(winning_mean_f1 - 1.96 * std_f1),\n",
    "           float(winning_mean_f1 + 1.96 * std_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our Top 10 Features\n",
    "These features have the highest feature importance scores as found by our best forest model.\n",
    "\n",
    "Unsurprisingly, they tend to include our most general metrics of performance, like average proficiency and grade 7 ela scores of 4 across all students.\n",
    "\n",
    "It is interesting to see how heavily the absence and attendance rates factor in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_math_proficiency</th>\n",
       "      <td>0.175213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_7_ela_4s_all_students</th>\n",
       "      <td>0.118439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_7_math_4s_all_students</th>\n",
       "      <td>0.099211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_of_students_chronically_absent</th>\n",
       "      <td>0.045525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_hispanic</th>\n",
       "      <td>0.034358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_attendance_rate</th>\n",
       "      <td>0.026382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school_pupil_teacher_ratio</th>\n",
       "      <td>0.025499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_class_size_english</th>\n",
       "      <td>0.018679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_class_size_social_studies</th>\n",
       "      <td>0.015125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strong_family_community_ties_percent</th>\n",
       "      <td>0.014503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Importances\n",
       "average_math_proficiency                   0.175213\n",
       "grade_7_ela_4s_all_students                0.118439\n",
       "grade_7_math_4s_all_students               0.099211\n",
       "percent_of_students_chronically_absent     0.045525\n",
       "percent_hispanic                           0.034358\n",
       "student_attendance_rate                    0.026382\n",
       "school_pupil_teacher_ratio                 0.025499\n",
       "average_class_size_english                 0.018679\n",
       "average_class_size_social_studies          0.015125\n",
       "strong_family_community_ties_percent       0.014503"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train_prepped.columns\n",
    "feature_importances = best_forest.feature_importances_\n",
    "\n",
    "features_and_importances = pd.DataFrame(feature_importances,features,['Importances'])\n",
    "\n",
    "# Need column names here after the ohe_data step to analyze the results\n",
    "features_and_importances.sort_values('Importances', ascending=False).iloc[1:11,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing a few trees from our forest\n",
    "\n",
    "To see what decisions some of our trees are coming to, let's take a look at three random trees out of our group of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trees_in_forest = best_forest.estimators_\n",
    "max_index = len(trees_in_forest)\n",
    "\n",
    "random_indeces = np.random.randint(0, max_index, 3)\n",
    "example_graphs = []\n",
    "\n",
    "for index in random_indeces:\n",
    "    tree_viz = export_graphviz(trees_in_forest[index], proportion=True, filled=True,\n",
    "                               feature_names=train_prepped.columns, rounded=True,\n",
    "                               class_names=['not_high_registrations','high_registrations'],\n",
    "                               out_file=None)\n",
    "    try:\n",
    "        graphviz.Source(source=tree_viz,\n",
    "                        filename='cache_forest/tree_viz_{0}'.format(index),\n",
    "                        format='svg').render()\n",
    "    except ExecutableNotFound:\n",
    "        print(\"Your system lacks GraphViz. Instructions to install for your\" + \\\n",
    "            \"operating system should be available at https://graphviz.gitlab.io/download/\" + \\\n",
    "            \"The images will be loaded and linked to below, so you don't need it to view\" + \\\n",
    "            \"this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the displayed graphs below, the more orange a cell is, the more the samples that pass through it tend to be not in our \"high_registrations\" category. The more blue a cell is, the more it tends to include \"high_registrations.\" We are using the gini measurement of impurity to structure our trees.\n",
    "\n",
    "The samples percentage tells us what percentage of our total samples pass through this node.\n",
    "\n",
    "The value list tells us how many of the samples that have reached this node are in each class. So the first value (value[0]) indicates what proportion of the samples in the node are not high_registrations, and the second value (value[1]) tells us how many are high_registrations. You can see that these values correspond to the coloring of the graph.\n",
    "\n",
    "Then from each node, if a sample meets the condition that titles the node, it travels to the lower left branch. If it does not meed the condition of the node, it travels down the right branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #13\n",
    "\n",
    "![Graph #13](cache_forest/tree_viz_13.svg)\n",
    "\n",
    "[Link to Graph #13 if not rendering on GitHub](https://www.dropbox.com/s/vqg9hm8ol2kxy7d/tree_viz_13.svg?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #39\n",
    "\n",
    "![Graph #39](cache_forest/tree_viz_39.svg)\n",
    "\n",
    "[Link to Graph #39 if not rendering on GitHub](https://www.dropbox.com/s/x0ny1fpk13yj16c/tree_viz_39.svg?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Tree #77\n",
    "\n",
    "![Graph #39](cache_forest/tree_viz_77.svg)\n",
    "\n",
    "[Link to Graph #77 if not rendering on GitHub](https://www.dropbox.com/s/rlspal912qu0euf/tree_viz_77.svg?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember these are just three out of our total 100 trees that make our ensemble predictor, and each of these trees only have half of the total features in our set. Their variation is what helps 'smooth out the edges' of some of the predictions, to gain the benefits of an ensemble within a single model.\n",
    "\n",
    "All in all, the graph results are to be expected given the features that we found to be important, but the PASSNYC team specifically asked for models that could be explained, and we feel these trees would of course help explain the model's decision-making process clearly to all stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring results on the test set\n",
    "\n",
    "Now that we have determined our best preprocessing steps and hyperparameters,\n",
    "we evaluate our results on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7083\n",
      "Accuracy: 0.8495\n"
     ]
    }
   ],
   "source": [
    "# We train on our full training data on a new forest with our best_params\n",
    "# determined by our GridSearchCV\n",
    "best_forest.fit(train_prepped, train_labels)\n",
    "predictions = best_forest.predict(test_prepped)\n",
    "\n",
    "# And make predictions on our test data\n",
    "# predictions = best_forest.predict(test_prepped)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "    \n",
    "print(\"Average F1 Score: {0:.4f}\".format(f1))\n",
    "print(\"Accuracy: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations Based on Opportunity for Engaging with Black/Hispanic Student Populations\n",
    "\n",
    "We will make our final recommendations based on the ranking methods described in our [overview notebook](final_project_overview.ipynb) that seek to identify the greatest opportunities for increasing SHSAT registrations at schools with high black and hispanic populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>1s</th>\n",
       "      <th>dbn</th>\n",
       "      <th>school_name</th>\n",
       "      <th>economic_need_index</th>\n",
       "      <th>grade_7_enrollment</th>\n",
       "      <th>num_shsat_test_takers</th>\n",
       "      <th>pct_test_takers</th>\n",
       "      <th>percent_black__hispanic</th>\n",
       "      <th>minority_delta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22K278</td>\n",
       "      <td>J.H.S. 278 MARINE PARK</td>\n",
       "      <td>0.494</td>\n",
       "      <td>428.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>76</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27Q210</td>\n",
       "      <td>J.H.S. 210 ELIZABETH BLACKWELL</td>\n",
       "      <td>0.602</td>\n",
       "      <td>643.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>106</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>08X337</td>\n",
       "      <td>THE SCHOOL FOR INQUIRY AND SOCIAL JUSTICE</td>\n",
       "      <td>0.781</td>\n",
       "      <td>181.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>51</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27Q137</td>\n",
       "      <td>M.S. 137 AMERICA'S SCHOOL OF HEROES</td>\n",
       "      <td>0.541</td>\n",
       "      <td>617.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28Q217</td>\n",
       "      <td>J.H.S. 217 ROBERT A. VAN WYCK</td>\n",
       "      <td>0.628</td>\n",
       "      <td>533.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53</td>\n",
       "      <td>47.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15K088</td>\n",
       "      <td>J.H.S. 088 PETER ROUGET</td>\n",
       "      <td>0.719</td>\n",
       "      <td>466.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>53</td>\n",
       "      <td>47.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27Q202</td>\n",
       "      <td>J.H.S. 202 ROBERT H. GODDARD</td>\n",
       "      <td>0.593</td>\n",
       "      <td>350.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30Q230</td>\n",
       "      <td>I.S. 230</td>\n",
       "      <td>0.601</td>\n",
       "      <td>439.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>38</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30Q010</td>\n",
       "      <td>I.S. 010 HORACE GREELEY</td>\n",
       "      <td>0.605</td>\n",
       "      <td>233.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>36</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10X141</td>\n",
       "      <td>RIVERDALE / KINGSBRIDGE ACADEMY (MIDDLE SCHOOL...</td>\n",
       "      <td>0.418</td>\n",
       "      <td>249.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank    1s     dbn                                        school_name  \\\n",
       "107     1  10.0  22K278                             J.H.S. 278 MARINE PARK   \n",
       "56      2   5.0  27Q210                     J.H.S. 210 ELIZABETH BLACKWELL   \n",
       "416     3  10.0  08X337          THE SCHOOL FOR INQUIRY AND SOCIAL JUSTICE   \n",
       "52      4  10.0  27Q137                M.S. 137 AMERICA'S SCHOOL OF HEROES   \n",
       "48      5   9.0  28Q217                      J.H.S. 217 ROBERT A. VAN WYCK   \n",
       "59      6   9.0  15K088                            J.H.S. 088 PETER ROUGET   \n",
       "65      7  10.0  27Q202                       J.H.S. 202 ROBERT H. GODDARD   \n",
       "45      8  10.0  30Q230                                           I.S. 230   \n",
       "445     9  10.0  30Q010                            I.S. 010 HORACE GREELEY   \n",
       "71     10  10.0  10X141  RIVERDALE / KINGSBRIDGE ACADEMY (MIDDLE SCHOOL...   \n",
       "\n",
       "     economic_need_index  grade_7_enrollment  num_shsat_test_takers  \\\n",
       "107                0.494               428.0                   98.0   \n",
       "56                 0.602               643.0                  169.0   \n",
       "416                0.781               181.0                   39.0   \n",
       "52                 0.541               617.0                  179.0   \n",
       "48                 0.628               533.0                  175.0   \n",
       "59                 0.719               466.0                  163.0   \n",
       "65                 0.593               350.0                   98.0   \n",
       "45                 0.601               439.0                  157.0   \n",
       "445                0.605               233.0                   56.0   \n",
       "71                 0.418               249.0                   75.0   \n",
       "\n",
       "     pct_test_takers  percent_black__hispanic  minority_delta  score  \n",
       "107             33.0                     63.0              76   76.0  \n",
       "56              26.0                     67.0             106   53.0  \n",
       "416             24.0                     96.0              51   51.0  \n",
       "52              27.0                     37.0              50   50.0  \n",
       "48              31.0                     55.0              53   47.7  \n",
       "59              39.0                     71.0              53   47.7  \n",
       "65              26.0                     55.0              44   44.0  \n",
       "45              36.0                     57.0              38   38.0  \n",
       "445             22.0                     58.0              36   36.0  \n",
       "71              15.0                     66.0              34   34.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_df = run_model_get_ordered_predictions(best_forest, train_data, test_data,\n",
    "                                      train_prepped, test_prepped,\n",
    "                                      train_labels, test_labels)\n",
    "\n",
    "\n",
    "# We now use another util function to generate the list we'll feed to our final\n",
    "# ensemble evaluation.\n",
    "\n",
    "df_passnyc = create_passnyc_list(fp_df, train_data, test_data, train_labels, test_labels)\n",
    "# Write to CSV\n",
    "df_passnyc.to_csv('results/results.randomforest.csv')\n",
    "\n",
    "df_passnyc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
